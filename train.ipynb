{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65e5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import FaceDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a519b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Датасет \n",
    "    Ожидает, что в папке root находятся изображения (в корне или в images/) и\n",
    "    файлы аннотаций .txt с таким же именем, как у изображения.\n",
    "    Исходный формат строки в .txt: class_id cx cy w h   (все нормализованы в [0,1])\n",
    "    Если в файле несколько линий (несколько объектов), по умолчанию берётся рамка\n",
    "    с максимальной площадью (w*h).\n",
    "    \"\"\"\n",
    "\n",
    "    IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n",
    "\n",
    "    def __init__(self, root=\"data\", img_size=256, transform=None, take_largest=True, augment=False):\n",
    "        self.root = root\n",
    "        self.img_size = img_size\n",
    "        self.take_largest = take_largest\n",
    "        self.augment = augment\n",
    "        \n",
    "        if transform is None:\n",
    "            base_transforms = [T.Resize((img_size, img_size))]\n",
    "            if augment:\n",
    "                # Аугментации для обучения\n",
    "                base_transforms = [\n",
    "                    T.RandomRotation(degrees=15),\n",
    "                    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "                    T.Resize((img_size, img_size)),\n",
    "                ]\n",
    "            else:\n",
    "                base_transforms = [T.Resize((img_size, img_size))]\n",
    "            base_transforms.append(T.ToTensor())\n",
    "            self.transform = T.Compose(base_transforms)\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "        possible_dirs = [os.path.join(root, \"images\"), root]\n",
    "        self.images = []\n",
    "        for d in possible_dirs:\n",
    "            if os.path.isdir(d):\n",
    "                for fn in os.listdir(d):\n",
    "                    if fn.lower().endswith(self.IMG_EXTS):\n",
    "                        self.images.append(os.path.join(d, fn))\n",
    "        self.images.sort()\n",
    "\n",
    "    def _label_path_for_image(self, img_path):\n",
    "        base, _ = os.path.splitext(img_path)\n",
    "        candidates = [\n",
    "            base + \".txt\",\n",
    "            os.path.join(self.root, \"labels\", os.path.basename(base) + \".txt\"),\n",
    "            os.path.join(self.root, os.path.basename(base) + \".txt\"),\n",
    "        ]\n",
    "        for p in candidates:\n",
    "            if os.path.isfile(p):\n",
    "                return p\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label_path = self._label_path_for_image(img_path)\n",
    "        if label_path is None:\n",
    "            raise FileNotFoundError(f\"Label file not found for image {img_path}\")\n",
    "\n",
    "        bboxes = []\n",
    "        with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                try:\n",
    "                    cls = int(float(parts[0]))\n",
    "                    cx = float(parts[1])\n",
    "                    cy = float(parts[2])\n",
    "                    w = float(parts[3])\n",
    "                    h = float(parts[4])\n",
    "                    bboxes.append((cls, cx, cy, w, h))\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        if len(bboxes) == 0:\n",
    "            raise ValueError(f\"No valid bbox in label file {label_path}\")\n",
    "\n",
    "        if self.take_largest and len(bboxes) > 1:\n",
    "            areas = [bb[3] * bb[4] for bb in bboxes]\n",
    "            idx_max = int(np.argmax(areas))\n",
    "            chosen = bboxes[idx_max]\n",
    "        else:\n",
    "            chosen = bboxes[0]\n",
    "\n",
    "        _, cx, cy, w, h = chosen\n",
    "        img_t = self.transform(img)\n",
    "        bbox = torch.tensor([cx, cy, w, h], dtype=torch.float32)\n",
    "        return img_t, bbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb51a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, opt, loss_fn, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    # Progress bar для тренировочных батчей\n",
    "    train_pbar = tqdm(dataloader, desc=f\"Train Epoch {epoch}\", leave=False)\n",
    "    for i, (imgs, targets) in enumerate(train_pbar):\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss = loss_fn(preds, targets)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        # Обновляем описание для отображения текущего loss\n",
    "        train_pbar.set_postfix({'batch_loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    train_pbar.close()\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def iou_xyxy(boxA, boxB):\n",
    "    # Вычисление метрики IoU для рамки в формате [x1,y1,x2,y2]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interW = max(0.0, xB - xA)\n",
    "    interH = max(0.0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "\n",
    "    boxAArea = max(0.0, boxA[2] - boxA[0]) * max(0.0, boxA[3] - boxA[1])\n",
    "    boxBArea = max(0.0, boxB[2] - boxB[0]) * max(0.0, boxB[3] - boxB[1])\n",
    "\n",
    "    denom = boxAArea + boxBArea - interArea\n",
    "    if denom <= 0:\n",
    "        return 0.0\n",
    "    return interArea / denom\n",
    "\n",
    "\n",
    "def cxcywh_to_xyxy_norm(cx, cy, w, h):\n",
    "    # Преобразование нормализованных координат cx,cy,w,h в нормализованные x1,y1,x2,y2\n",
    "    x1 = cx - w / 2.0\n",
    "    y1 = cy - h / 2.0\n",
    "    x2 = cx + w / 2.0\n",
    "    y2 = cy + h / 2.0\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "\n",
    "def xyxy_norm_to_pixels(box, img_w, img_h):\n",
    "    # Преобразует нормализованные xyxy в пиксели (int)\n",
    "    x1 = int(round(box[0] * img_w))\n",
    "    y1 = int(round(box[1] * img_h))\n",
    "    x2 = int(round(box[2] * img_w))\n",
    "    y2 = int(round(box[3] * img_h))\n",
    "    x1 = max(0, min(x1, img_w - 1))\n",
    "    x2 = max(0, min(x2, img_w - 1))\n",
    "    y1 = max(0, min(y1, img_h - 1))\n",
    "    y2 = max(0, min(y2, img_h - 1))\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    data_root = \"dataset\"\n",
    "\n",
    "    train_dir = os.path.join(data_root, \"train\")\n",
    "    val_dir = os.path.join(data_root, \"val\")\n",
    "\n",
    "    # Датасеты с аугментацией для обучения, без аугментации для валидации\n",
    "    train_data = FaceDataset(\n",
    "        root=train_dir, img_size=256, transform=None, take_largest=True, augment=True\n",
    "    )\n",
    "    val_data = FaceDataset(\n",
    "        root=val_dir, img_size=256, transform=None, take_largest=True, augment=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = FaceDetector()\n",
    "    model.to(device)\n",
    "\n",
    "    # SmoothL1Loss более устойчив к выбросам, чем L1Loss\n",
    "    loss_fn = nn.SmoothL1Loss(beta=0.1)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=20, gamma=0.5)\n",
    "\n",
    "    epochs = 100\n",
    "    best_val_iou = -1\n",
    "    best_path = \"best_model.pt\"\n",
    "    patience = 40\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # УБРАЛ главный progress bar для эпох - используем обычный цикл\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        \n",
    "        # Только ОДИН progress bar для тренировки\n",
    "        train_loss = 0.0\n",
    "        model.train()\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Training\")\n",
    "        for i, (imgs, targets) in enumerate(train_pbar):\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss += loss.item() * imgs.size(0)\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        train_pbar.close()\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Валидация БЕЗ progress bar (простой цикл)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        iou_total = 0.0\n",
    "        count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # УБРАЛ val_pbar - используем обычный цикл\n",
    "            for imgs, targets in val_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                preds = model(imgs)\n",
    "                loss = loss_fn(preds, targets)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "                for p, t in zip(preds.cpu().numpy(), targets.cpu().numpy()):\n",
    "                    p_xyxy = cxcywh_to_xyxy_norm(p[0], p[1], p[2], p[3])\n",
    "                    t_xyxy = cxcywh_to_xyxy_norm(t[0], t[1], t[2], t[3])\n",
    "                    iou = iou_xyxy(p_xyxy, t_xyxy)\n",
    "                    iou_total += iou\n",
    "                    count += 1\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        avg_iou = iou_total / max(1, count)\n",
    "        \n",
    "        print(f\"Train loss: {train_loss:.6f}  Val loss: {val_loss:.6f}  Val IoU: {avg_iou:.4f}\")\n",
    "\n",
    "        if avg_iou > best_val_iou:\n",
    "            best_val_iou = avg_iou\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"✓ Saved best model to {best_path} (IoU: {avg_iou:.4f})\")\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Early stopping если IoU не улучшается\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping: no improvement for {patience} epochs\")\n",
    "            break\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"\\nTraining finished. Best val IoU: {best_val_iou:.4f}\")\n",
    "    print(f\"Best model saved to: {best_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd978be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 146/146 [00:40<00:00,  3.62it/s, loss=0.0212]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.021258  Val loss: 0.012495  Val IoU: 0.4519\n",
      "✓ Saved best model to best_model.pt (IoU: 0.4519)\n",
      "\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 146/146 [00:29<00:00,  4.88it/s, loss=0.0127]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.012779  Val loss: 0.013081  Val IoU: 0.4346\n",
      "\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 146/146 [00:30<00:00,  4.86it/s, loss=0.0185]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.011891  Val loss: 0.010712  Val IoU: 0.5187\n",
      "✓ Saved best model to best_model.pt (IoU: 0.5187)\n",
      "\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 95/146 [00:20<00:11,  4.60it/s, loss=0.0077]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
